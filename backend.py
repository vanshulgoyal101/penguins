# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11oeHRK_u-qAXFsVi6cfn9tXwoCNWNhPX

    Author:- Sexy_Penguin
"""
import collections
import pandas as pd
import wordfreq
import pickle
import numpy as np
import matplotlib.pyplot as plt
import requests
import json
import urllib
from collections import namedtuple
from nltk import word_tokenize
from functools import lru_cache
import re
import unicodedata
import sys
from collections import Counter
import nltk
nltk.download('brown')
from nltk.corpus import brown
from nltk import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk import pos_tag
from wordfreq import zipf_frequency
nltk.download('averaged_perceptron_tagger')
import torch
from transformers import BertTokenizer, BertModel, BertForMaskedLM

bert_model = 'bert-large-uncased'
tokenizer = BertTokenizer.from_pretrained(bert_model)
model = BertForMaskedLM.from_pretrained(bert_model)
pickle.dump(model,open('model.pkl','wb'))
pickle.dump(tokenizer,open('tokenizer.pkl','wb'))
def get_bert_candidates(input_text, list_cwi_predictions, numb_predictions_displayed = 5):
  list_candidates_bert = []
  for word,pred  in zip(input_text.split(),list_cwi_predictions):
    if (pred and (pos_tag([word])[0][1] in ['NNS', 'NN', 'VBP', 'RB', 'VBG','VBD' ]))  or (zipf_frequency(word, 'en')) <3.1:
      replace_word_mask = input_text.replace(word, '[MASK]')
      text = f'[CLS]{replace_word_mask} [SEP] {input_text} [SEP] '
      tokenized_text = tokenizer.tokenize(text)
      masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]
      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
      segments_ids = [0]*len(tokenized_text)
      tokens_tensor = torch.tensor([indexed_tokens])
      segments_tensors = torch.tensor([segments_ids])
      # Predict all tokens
      with torch.no_grad():
          outputs = model(tokens_tensor, token_type_ids=segments_tensors)
          predictions = outputs[0][0][masked_index]
      predicted_ids = torch.argsort(predictions, descending=True)[:numb_predictions_displayed]
      predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))
      list_candidates_bert.append((word, predicted_tokens))
  return list_candidates_bert

from keybert import KeyBERT
doc = input("Please Enter some search words:")
kw_model = KeyBERT()
limst=doc.split()
if len(limst)<3:
  top_n=len(limst)
else:
  top_n=3

keywords = kw_model.extract_keywords(doc)[:][0:top_n]
list_p=[]
for i in range(0,top_n):
  list_p.append(keywords[i][0])

list_p

list_p
stri=""
for i in range(0,top_n):
  stri = stri + " "+ list_p[i]

stri=stri.strip().lower()
stri

arr= get_bert_candidates(doc,list_p)

len(arr)

arr

farr=[]
for i in range(0,len(arr)):
  farr.append(arr[i][1][0])

farr
stri2=f"{farr[0]}"
for i in range(1,len(farr)):
  stri2=stri2+", "+farr[i]
y_a=2015
y_b=2016
parameters={
  "q":doc,
  "keywords":stri2,
  "year_start":y_a,
  "year_end":y_b,
  "media_type":"image"
}
response_API = requests.get("https://images-api.nasa.gov/search", params=parameters)
print(response_API.status_code)
data = response_API.text
parse_json= json.loads(data)
transfer=parse_json["collection"]["items"]
print(transfer)

json_object = json.dumps(transfer)
with open("sample.json", "w") as outfile:
    outfile.write(json_object)

    